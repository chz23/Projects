{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87eace1-dd44-4745-9407-dd75fb9c2a38",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Natural Language Processing - Classifying Definitions of Nouns and Adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9032452-62eb-477f-9423-7b9831a3c668",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Background, Overview & Data Scraping\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e2b6c-268f-4bea-9f21-72459f7fd84a",
   "metadata": {},
   "source": [
    "## Contents\n",
    "---\n",
    "- [Problem Statement](##Problem-Statement)\n",
    "- [Overview of Process](##Overview-of-Process)\n",
    "- [Introduction to NLP Topic](##Introduction-to-NLP-Topic)\n",
    "- [Considerations for Data Scraping](##Considerations-for-Data-Scraping)\n",
    "- [Data Scraping](##Data-Scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ca8db-9aab-437d-811a-582ebca18605",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem Statement\n",
    "---\n",
    "\n",
    "Constant exposure to a language through conversation, reading and writing is often the best long-term strategy to gain mastery. However, formal curricula and teaching methods are required to be explicit and objective-driven, thus leading to a structured, rules-based approach that is often seen as stifling and/or boring. To combat this, models can be built to mimic video games, where learners ‘compete’ against models. This may help learners view learning more positively by incentivising them to work harder so as to be able to win. This project represents the starting point, and it aims to use Natural Language Processing (NLP) to develop a model that can classify a given definition as either defining a noun or an adjective. Educators will be able to input word definitions and pit learners against the model to see who has a higher accuracy rate. In addition, much like how video games feature different levels of difficulty, the model can easily be tuned to the desired level of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f26b3-0db1-4872-a7d8-ae1e917f2591",
   "metadata": {},
   "source": [
    "---\n",
    "## Overview of Process\n",
    "---\n",
    "\n",
    "#### Introduction & Data Scraping\n",
    "\n",
    "This segment explores the objective of this project, how it compares to other NLP projects, as well as potential limitations and considerations. \n",
    "\n",
    "Following this, data scraping is carried out. Rationales for the websites used, as well as conditions put in place while scraping, are elaborated upon in the relevant sections of this document.\n",
    "\n",
    "\n",
    "#### Exploratory Data Analysis (EDA)\n",
    "\n",
    "Preliminary EDA is carried out in 3 main parts:\n",
    "- length of definitions;\n",
    "- prominent parts-of-speech (POS) by word class;\n",
    "- prominent features by word class.\n",
    "\n",
    "\n",
    "#### Modelling\n",
    "\n",
    "Based on a mixture of the EDA, logical reasoning as well as domain knowledge, data will be processed and models tuned accordingly.\n",
    "\n",
    "A total of 4 models were explored: \n",
    "- Naive Bayes (NB) (part of the requirements for this project)\n",
    "- Logistic Regression (LR)\n",
    "- K-Nearest Neighbours (KNN)\n",
    "- Random Forest (RF)\n",
    "\n",
    "Prior to modelling, a brief overview of each model will be given, including potential advantages and limitations in the context of this project.\n",
    "\n",
    "The models will then be assessed based on the following sets of scores:\n",
    "| Metric | Applicable Dataset |\n",
    "| --- | --- |\n",
    "| CV | training |\n",
    "| accuracy | training & test |\n",
    "| specificity | test |\n",
    "| recall | test |\n",
    "| precision | test |\n",
    "| f1 | test |\n",
    "\n",
    "\n",
    "#### Explanatory Data Analysis (ExDA)\n",
    "\n",
    "After the final models are selected, explanatory data analysis will be carried out to explain how the models work and how some important differences may lead them to different conclusions.\n",
    "\n",
    "\n",
    "#### Applications, Suggestions & Recommendations\n",
    "\n",
    "This final segment covers possible applications of the models, as well as potential extensions, future versions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ecf613-e8e0-433c-b9e0-1242d01a7a2d",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction to NLP Topic\n",
    "---\n",
    "\n",
    "#### Objective\n",
    "\n",
    "This NLP project aims to take in definitions of English words, specifically that of nouns and adjectives, and classify them as defining either nouns or adjectives. All definitions are taken from the same [source](https://www.dictionary.com). \n",
    "\n",
    "\n",
    "#### Comparison with other NLP projects\n",
    "\n",
    "Compared to other NLP projects that scrape data from forums, reviews, etc, this project is a lot more contained and structured since it uses formal definitions. On one hand, this could be a boon, as professionally-written definitions are likely to follow certain structures and rules, thus making it easier for the model to train on. On the other hand, the data scraped is a lot more ‘robotic’ than posts written by human beings, which could make modelling more challenging - lengths are unlikely to differ much, ‘tone’ is irrelevant, etc.\n",
    "\n",
    "\n",
    "#### Considerations & limitations\n",
    "\n",
    "In terms of data, this project is both aided and limited by the consistency of data. As it is a small-scale project, care has been taken to ensure that the models can work with the data - words scraped are largely common, definitions are from the same website, and certain conditions have been put in place to eliminate largely useless information, such as overly-short definitions of 5 or fewer words. This makes the data more reliable, but also limits its use. We will explore this more in the final notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22acf406-6de3-411f-b779-9d8d6bb4e10f",
   "metadata": {},
   "source": [
    "---\n",
    "## Considerations for Data Scraping\n",
    "---\n",
    "\n",
    "#### Data sources\n",
    "\n",
    "Words come from two different lists that purport to be that of commonly-used nouns and adjectives. While users are not required to understand definitions to be able to classify them, it is likely that definitions for uncommon words would be more difficult to understand as well. Therefore, using common words is a better idea. \n",
    "\n",
    "The definitions come from dictionary.com. This is an often-used and reputable dictionary. Cambridge would have been a better choice, but the website disallows scraping. \n",
    "\n",
    "\n",
    "#### Conditions put in place during the scraping process\n",
    "\n",
    "Definitions that fulfilled the following conditions were scraped:\n",
    "- Consisting of at least 5 words: overly-succinct definitions are often not definitions, but rather synonyms;\n",
    "- Formal definitions: dictionary.com occasionally provides informal definitions, likely for the sake of completeness, but sticking to standard English is better, especially if the model is intended for further uses;\n",
    "- Are still in use: some words are archaic, as noted by dictionary.com, and should not be scraped.\n",
    "\n",
    "\n",
    "#### 'Re'-scraping\n",
    "\n",
    "Words tend to span mutiple classes. As such, it is possible to use the nouns list to scrape definitions, and vice-versa. The reason this is necessary is because less adjectives made the cut according to the conditions above. \n",
    "\n",
    "\n",
    "#### Data cleaning\n",
    "\n",
    "Scraping two rounds from both lists inevitably results in some duplicated entries that have to be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a148e266-22a9-4724-afce-40d7959f20cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e440f79-f19e-4233-9cfb-979f9d18d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that, for a given word class (noun or adjective):\n",
    "# 1. scrapes words from a given URL, then\n",
    "# 2. scrapes their definitions from dictionary.com.\n",
    "\n",
    "class Scrape():\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 url=None, \n",
    "                 class_word=None, \n",
    "                 class_def=None, \n",
    "                 limit=False):\n",
    "        \n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        url : str\n",
    "            URL to scrape words from.\n",
    "        class_word : str\n",
    "            Accepts 'noun' or 'adjective'. \n",
    "            Used to differentiate between which scraping code should be used.\n",
    "        class_def : str\n",
    "            Accepts 'noun' or 'adjective'.\n",
    "            Used to indicate which type of definition should be scraped.\n",
    "        '''\n",
    "        \n",
    "        self.url = url\n",
    "        self.class_word = class_word\n",
    "        self.class_def = class_def\n",
    "        self.limit = limit\n",
    "        \n",
    "        self.get_words()\n",
    "        self.get_definitions()\n",
    "        self.create_df()\n",
    "     \n",
    "    \n",
    "    # Function to scrape words from a given URL.\n",
    "    def get_words(self):\n",
    "        \n",
    "        # List to store words.\n",
    "        self.words = []\n",
    "\n",
    "        response = requests.get(self.url)\n",
    "        \n",
    "        html = response.text\n",
    "        \n",
    "        soup = BeautifulSoup(html, features='lxml')\n",
    "        \n",
    "        # Scrape only definitions that match the specified word class.\n",
    "        # Note that the following code is specific to the webpage.\n",
    "        if self.class_word == 'noun':\n",
    "            \n",
    "            # Each 'tr' tag corresponds to a row of 3 words in this webpage.\n",
    "            for row in soup.findAll('tr'):\n",
    "                \n",
    "                # Each 'td' tag corresponds to 1 of the 3 words, as well as its number.\n",
    "                for word_and_num in row.findAll('td'):\n",
    "                    \n",
    "                    # Eliminates the word's number.\n",
    "                    for word in word_and_num.findAll('a'):\n",
    "                        \n",
    "                        try:\n",
    "                            \n",
    "                            # Words are preceded by \\r & \\n.\n",
    "                            # .split() is applied to scrape only the actual word.\n",
    "                            self.words.append(word.text.lower().split('\\r\\n')[1])\n",
    "                            \n",
    "                        except:\n",
    "                            \n",
    "                            pass\n",
    "\n",
    "        # Scrape only definitions that match the specified word class.\n",
    "        # Note that the following code is specific to the webpage.\n",
    "        elif self.class_word == 'adjective':\n",
    "            \n",
    "            # Each 'tbody' tag corresponds to a table of words.\n",
    "            # Each table corresponds to a letter of the alphabet.\n",
    "            for table in soup.findAll('tbody'):\n",
    "                \n",
    "                # Each 'td' tag corresponds to 1 word in the table.\n",
    "                for word in table.findAll('td'):\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Words are preceded and succeeded by a whitespace.\n",
    "                        # .split() is applied to scrape only the actual word.\n",
    "                        self.words.append(word.text.lower().split(' ')[1])\n",
    "                        \n",
    "                    except:\n",
    "                        \n",
    "                        pass\n",
    "\n",
    "        return self.words\n",
    "    \n",
    "    \n",
    "    # Function to scrape definitions from dictionary.com.\n",
    "    # To avoid confusion, some for-loops will be labelled (for eg. Loop [1]) and referenced later on.\n",
    "    def get_definitions(self):\n",
    "        \n",
    "        # Dictionary that contains the following key-value pairs:\n",
    "        # Key: word.\n",
    "        # Value: list, where the 1st value is the specific dictionary.com URL for the word.\n",
    "        # The remaining list elements will be added later on.\n",
    "        self.info = {self.words[i]: [f'https://www.dictionary.com/browse/{self.words[i]}'] \\\n",
    "                     for i in range(len(self.words))}\n",
    "        \n",
    "        if self.limit:\n",
    "            \n",
    "            for word in list(self.info.keys())[self.limit:]:\n",
    "                del self.info[word]\n",
    "        \n",
    "        for word, lst in self.info.items():\n",
    "            \n",
    "            # There's a possibility that dictionary.com does not contain that specific word.\n",
    "            try:\n",
    "\n",
    "                response = requests.get(lst[0])\n",
    "\n",
    "            except:\n",
    "\n",
    "                continue\n",
    "\n",
    "            html = response.text\n",
    "\n",
    "            soup = BeautifulSoup(html, features='lxml')\n",
    "\n",
    "            # Loop [1]\n",
    "            # Each dictionary.com page is separated into sections.\n",
    "            # Each section contains definitions for a given word class.\n",
    "            # Word classes are repeated as there are 2 major segments for each page...\n",
    "            # ...American & British English.\n",
    "            # For example, if the word can be a noun, there would be two 'noun' sections.\n",
    "            for section in soup.findAll('section', \n",
    "                                        attrs={'class': 'css-109x55k e1hk9ate4'}):\n",
    "\n",
    "                # Loop [2]\n",
    "                # Each 'span' tag, with the specified attribute, corresponds to a word class.\n",
    "                for w_class in section.findAll('span', \n",
    "                                               attrs={'class': 'luna-pos'}):\n",
    "\n",
    "                    # Word classes are expressed in a variety of ways in dictionary.com...\n",
    "                    # ...but they always start with the actual word class itself.\n",
    "                    if self.class_def in w_class.text:\n",
    "\n",
    "                        # Loop [3]\n",
    "                        # Each 'div' tag, with the specified attribute, corresponds to either:\n",
    "                        # 1) an expandable section, or\n",
    "                        # 2) a non-expandable section.\n",
    "                        # These sections contain definition(s) of the word for the specified word class.\n",
    "                        for content in section.findAll('div', \n",
    "                                                       attrs={'class': 'css-10n3ydx e1hk9ate0'}):\n",
    "\n",
    "                            # For the expandable sections.\n",
    "                            # Expandable sections are defined as having a 'see more' option that...\n",
    "                            # ...can be clicked and collapsed.\n",
    "                            try:\n",
    "\n",
    "                                # If an exception is raised here, the section is non-expandable.\n",
    "                                assert content.findAll('div', \n",
    "                                                       attrs={'class': 'default-content'})\n",
    "\n",
    "                                # Each 'div' tag, with the specified attribute, corresponds to the...\n",
    "                                # ...default section, ie. what's already shown on the page.\n",
    "                                # Note: this loop doesn't require breaking, since there's only 1.\n",
    "                                for definitions in content.findAll('div', \n",
    "                                                                    attrs={'class': 'default-content'}):\n",
    "\n",
    "                                    # Loop [4a]\n",
    "                                    # Each 'div' tag corresponds to 1 definition.\n",
    "                                    for definition in definitions.findAll('div'):\n",
    "                                        \n",
    "                                        \n",
    "                                        # Only scrape definitions that fulfill the followng requirements:\n",
    "                                        # 1. at least 5 words, to avoid scraping mere synonyms;\n",
    "                                        # 2. are not informal definitions, to avoid non-standard English;\n",
    "                                        # 3. are still in use (not archaic).\n",
    "                                        if len(definition.text.split()) >= 5 and \\\n",
    "                                        not definition.text.lower().startswith('informal') and \\\n",
    "                                        not definition.text.lower().startswith('archaic'):\n",
    "\n",
    "                                            # As mentioned above, the dictionary will have additional elements.\n",
    "                                            # The 2nd element is the word class of the word.\n",
    "                                            self.info[word].append(self.class_def)\n",
    "\n",
    "                                            # The 3rd element is a definition of the word.\n",
    "                                            self.info[word].append(definition.text.lower())\n",
    "\n",
    "                                            # Breaks loop [4a].\n",
    "                                            # Ie. only the 1st relevant definition is scraped.\n",
    "                                            break\n",
    "                                            \n",
    "                                        else:\n",
    "                                            \n",
    "                                            continue\n",
    "\n",
    "                            # For the non-expandable sections.\n",
    "                            # Non-expandable sections are defined as not having the 'see more' option.\n",
    "                            # Ie. all definitions are already shown on the page.\n",
    "                            except:\n",
    "\n",
    "                                # Loop [4b]\n",
    "                                # Each 'div' tag corresponds to 1 definition.\n",
    "                                for definition in content.findAll('div'):\n",
    "                                    \n",
    "                                    # Only scrape definitions that fulfill the followng requirements:\n",
    "                                    # 1. at least 5 words, to avoid scraping mere synonyms;\n",
    "                                    # 2. are not informal definitions, to avoid non-standard English;\n",
    "                                    # 3. are still in use (not archaic).\n",
    "                                    if len(definition.text.split()) >= 5 and \\\n",
    "                                    not definition.text.lower().startswith('informal') and \\\n",
    "                                    not definition.text.lower().startswith('archaic'):\n",
    "\n",
    "                                        # As mentioned above, the dictionary will have additional elements.\n",
    "                                        # The 2nd element is the word class of the word.\n",
    "                                        self.info[word].append(self.class_def)\n",
    "\n",
    "                                        # The 3rd element is a definition of the word.\n",
    "                                        self.info[word].append(definition.text.lower())\n",
    "\n",
    "                                        # Breaks loop [4b].\n",
    "                                        # Ie. only the 1st relevant definition is scraped.\n",
    "                                        break\n",
    "                                        \n",
    "                                    else:\n",
    "                                        \n",
    "                                        continue\n",
    "\n",
    "                            # Length of list should be 3 (URL, word class, definition)...\n",
    "                            # ...if scraping was successful.\n",
    "                            if len(self.info[word]) == 3:\n",
    "\n",
    "                                # Breaks loop [3].\n",
    "                                # Avoids moving on to the next section.\n",
    "                                break\n",
    "\n",
    "                    if len(self.info[word]) == 3:\n",
    "\n",
    "                        # Breaks loop [2].\n",
    "                        # Avoids looking for word classes in other sections.\n",
    "                        break\n",
    "                        \n",
    "                if len(self.info[word]) == 3:\n",
    "\n",
    "                    # Breaks loop [1].\n",
    "                    # Avoids looking for word classes in other sections.\n",
    "                    break\n",
    "\n",
    "        return self.info\n",
    "    \n",
    "    \n",
    "    # Creates a DataFrame to store scraped information.\n",
    "    def create_df(self):\n",
    "        \n",
    "        self.df = pd.DataFrame.from_dict(self.info, \n",
    "                                         orient='index', \n",
    "                                         columns=['url', 'word_class', 'definition'])\n",
    "        \n",
    "        self.df.reset_index(inplace=True)\n",
    "        \n",
    "        self.df.rename({'index': 'word'}, axis=1, inplace=True)\n",
    "        \n",
    "        # Drop words that have no definitions for the specified word class.\n",
    "        self.df.dropna(inplace=True)\n",
    "        \n",
    "        # Unlikely to happen, but good to have as a precaution to ensure integrity of data.\n",
    "        self.df.drop_duplicates(subset=['definition'], inplace=True)\n",
    "        \n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9de313-34a4-438b-a1b2-96096930c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of webpages containing the nouns and adjectives for scraping.\n",
    "\n",
    "url_nouns = 'https://www.syllablecount.com/syllables/words/nouns.aspx'\n",
    "url_adjectives = 'https://www.wordscoach.com/blog/common-adjectives/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78fdcb8d-65f8-4515-b1d1-617ee43b1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Scrape() class, create a DataFrame of scraped nouns and their definitions.\n",
    "\n",
    "df_nouns_1 = Scrape(url=url_nouns, \n",
    "                    class_word='noun', \n",
    "                    class_def='noun', \n",
    "                    limit=False).df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62fe0286-6f48-4240-91fb-81e49f1df1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Scrape() class, create a DataFrame of scraped adjectives and their definitions.\n",
    "\n",
    "df_adjectives_1 = Scrape(url=url_adjectives, \n",
    "                         class_word='adjective', \n",
    "                         class_def='adjective', \n",
    "                         limit=False).df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ebc06c0-9396-4c87-846b-5a049c747a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many more nouns to scrape (includes buffer).\n",
    "\n",
    "shortage_nouns = 2000-df_nouns_1.shape[0]+200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e829781-5340-44b1-a575-e4f9f0085ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Scrape() class, create a 2nd DataFrame of scraped nouns and their definitions.\n",
    "\n",
    "df_nouns_2 = Scrape(url=url_adjectives, \n",
    "                    class_word='adjective', \n",
    "                    class_def='noun', \n",
    "                    limit=shortage_nouns).df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "745f7b78-7d18-4691-966e-8e7c63ed652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Scrape() class, create a 2nd DataFrame of scraped adjectives and their definitions.\n",
    "\n",
    "df_adjectives_2 = Scrape(url=url_nouns, \n",
    "                         class_word='noun', \n",
    "                         class_def='adjective').df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d45b26-6ee5-46e8-b9d6-ca247d573db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 1966)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check size of scraped data.\n",
    "\n",
    "df_nouns_1.shape[0]+df_nouns_2.shape[0], df_adjectives_1.shape[0]+df_adjectives_2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc358ed9-8e0e-404f-a900-67642ac0a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames together.\n",
    "\n",
    "df = pd.concat([df_nouns_1, df_nouns_2, df_adjectives_1, df_adjectives_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5deef489-fd4b-4957-b358-a256219037a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values, if any.\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6237052f-9c33-4be6-92ea-6cf7c798b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop repeated definitions.\n",
    "\n",
    "df = df.drop_duplicates(subset='definition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca6ec824-cec5-4b40-9f90-619d6cfd4967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "noun         0.515712\n",
       "adjective    0.484288\n",
       "Name: word_class, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that word classes are reasonably balances.\n",
    "\n",
    "df.word_class.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8b5e7ee-6e40-4bca-9462-56d9f1b706c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to ensure that numbers are sequential.\n",
    "\n",
    "df = df.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f060daf-fb51-44b0-8a66-2489fcb4bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame into csv file for use in other notebooks.\n",
    "\n",
    "df.to_csv('../data/nouns_and_adjectives.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c29691-ad35-4fe1-be17-950a42231cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi-sg]",
   "language": "python",
   "name": "conda-env-dsi-sg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
