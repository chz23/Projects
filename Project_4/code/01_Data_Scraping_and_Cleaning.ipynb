{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ba0bc6-0bc9-4ae8-9145-177f6fa6c494",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 4: Predicting Dengue Cases in Singapore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784d129e-9cac-434f-b029-b070bf08840a",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Background, Overview, Data Scraping and Data Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1e079-34d7-408c-b03a-dba0c4bc143f",
   "metadata": {},
   "source": [
    "## Contents\n",
    "---\n",
    "- [Problem Statement](##Problem-Statement)\n",
    "- [Background](##Background)\n",
    "- [Overview of Process](##Overview-of-Process)\n",
    "- [Data Scraping & Data Cleaning](##Data-Scraping-&-Data-Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac54a6-e2a7-4680-b604-94cc408b22c5",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem Statement\n",
    "---\n",
    "\n",
    "Dengue (dengue fever) is a disease that is endemic to Singapore. Despite constant efforts by the government and public to suppress its spread, Singapore continues to suffer from periodic outbreaks. In particular, outbreaks have been occurring at an increasing rate in recent years, with some years experiencing record-breaking numbers. Numerous factors are involved in influencing the spread of dengue and the resulting number of cases, thus making it difficult or virtually impossible to eradicate the virus; as a nation, we are constantly in 'defense-mode', ie., vector control measures are often carried out in a reactionary manner. In light of these challenges, our group has decided to tackle this challenge using a data-driven approach, where we aim to forecast future cases so as to provide an additional time advantage that we have over the virus. By doing so, we hope to imbue the National Environment Agency (NEA) with the confidence to carry out preventive measures at the earliest possible stage based on forecasts from our models, thus positively impacting the lives of Singaporeans as well as reducing associated costs with outbreaks, such as high costs incurred from aggressive vector control measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5e5b1-998d-45df-b47d-aca2671917c6",
   "metadata": {},
   "source": [
    "---\n",
    "## Overview of Process\n",
    "---\n",
    "\n",
    "#### Data Cleaning & Data Scraping\n",
    "\n",
    "This segment deals with scraping, cleaning and filtering of data. Challenges associated with doing so will be elaborated in the relevant sections below, as well as our approach to coming up with a reasonable solution.\n",
    "\n",
    "#### Exploratory Data Analysis (EDA)\n",
    "\n",
    "This segment covers the following:\n",
    "- Background information on dengue cases in Singapore;\n",
    "- Detailed look into factors contributing towards case numbers;\n",
    "- Analysis by planning area;\n",
    "- Cost-Benefit Analysis (CBA)\n",
    "\n",
    "#### Modelling\n",
    "\n",
    "This segment is split into two notebooks, with each notebook approaching the modelling process from a different point of view.\n",
    "\n",
    "While both approaches involve forecasting based on historical information, they differ in the exact methods as well as predictors being used.\n",
    "\n",
    "#### Applications, Suggestions & Recommendations\n",
    "\n",
    "Potential applications of the models are covered in this segment, as well as our suggestions on how the forecasting process can be improved upon to maximise obtainable benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb189696-f7b0-4d69-ab60-a4aa36b977aa",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Scraping & Data Cleaning\n",
    "---\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "| Data Type | Timeframe in Original Dataset | Weekly? | Monthly? | Yearly? |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Climate (temperature & rainfall) | Daily | Yes | Yes | Yes |\n",
    "| Infectious diseases (including dengue) | Weekly | Yes | Yes | Yes |\n",
    "| Google search trends | Weekly, Monthly | Yes | Yes | No$^1$ |\n",
    "| Outbound travel data | Monthly | Yes$^2$ | Yes | Yes |\n",
    "| Inbound travel data | Monthly | Yes$^2$ | Yes | Yes |\n",
    "\n",
    "$^1$ Weekly google search trends data for search term 'dengue'; note that it's not an absolute number but rather a representation, according to this definition given by Google:\n",
    "> Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means that there was not enough data for this term.\n",
    "\n",
    "Therefore, the data should be taken at face value. As such, there will be no yearly version of it since only weekly and monthly data can be extracted from the source.\n",
    "\n",
    "$^2$ Monthly travel data will be divided into the relevant number of weeks. While this may not be the most accurate way of getting weekly data, the data is crucial in explaining the COVID-period numbers, therefore we have decided to prioritise including it at the expense of some accuracy.\n",
    "\n",
    "#### Data Cleaning & Merging\n",
    "\n",
    "The 'end goal' is to have all the datasets compiled into weekly, monthly and yearly versions. To achieve this, the individual DataFrames must first be cleaned and standardised, and finally merged. Details specific to each dataset can be found in the relevant sections dealing with each individual dataset within this notebook.\n",
    "\n",
    "Each individual DataFrame will only contain relevant information. For example, the dengue DataFrame will only contain one column, `dengue_cases`.\n",
    "\n",
    "Most years have 52 weeks, with a few exceptions. Unfortunately, due to inherent differences of the source data, years with 53 weeks are not always the same across different datasets. In addition, if the 53rd week is only present in certain years, it may pose a problem in certain processes. Therefore, to minimise these issues, the 53rd week, when it exists, will be 'merged' into the 52nd week:\n",
    "- For data dealing with totals, such as total rainfall, `.sum()` will be applied.\n",
    "- For data dealing with averages, such as mean temperature, `.mean()` will be applied.\n",
    "- For data dealing with minimum values, such as minimum temperature, `.min()` will be applied.\n",
    "- For data dealing with maximum values, such as maximum temperature, `.max()` will be applied.\n",
    "- For 'special' data where it is inaccurate to apply mathematical operators, such as Google search trends, the 53rd week will be dropped.\n",
    "\n",
    "To ensure that the merger is successful, the indexes of all weekly / monthly / yearly Dataframes will be standardised. Examples are as follows:\n",
    "- Weekly data: 2012-W01\n",
    "- Monthly data: 2012-M01\n",
    "- Yearly data: 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25eb27b-f113-4d25-b271-4bfa5b720b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import calendar\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470b925e-89d7-47d3-b38d-690844ff06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a 'week' column based on the 'year' column and the consecutive nature of the data.\n",
    "\n",
    "def insert_week_col(df=None):\n",
    "    \n",
    "    '''   \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to be used in function.\n",
    "    '''\n",
    "    \n",
    "    # Set a placeholder value of 1 for all cells in the new 'week' column.\n",
    "    df['week'] = 1\n",
    "\n",
    "    # These variables will be used in the for loop below.\n",
    "    prev_year = None\n",
    "    prev_week = None\n",
    "\n",
    "    # Iterate over each row of the dataframe.\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        # If the 'year' value for the current row is different from the 'year' value for the previous row...\n",
    "        if prev_year is not None and row['year'] != prev_year:\n",
    "\n",
    "            # ...restart the numbering for the weeks.\n",
    "            prev_week = None\n",
    "\n",
    "        # If it is the start of a new year, retain the current 'week' value of 1.\n",
    "        # If not, set the current 'week' value to be the previous 'week' value + 1.\n",
    "        if prev_week is not None:\n",
    "\n",
    "            df.at[idx, 'week'] = prev_week + 1\n",
    "\n",
    "        # Update the previous year and previous week values to be the current values.\n",
    "        prev_year = df.at[idx, 'year']\n",
    "        prev_week = df.at[idx, 'week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7163cbcd-b1af-41af-b769-74fe767e545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to 'merge' data from the 53rd week into the 52nd week.\n",
    "\n",
    "def resolve_conflicts(df=None, year=None, column=None):\n",
    "    \n",
    "    '''   \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to be used in function.\n",
    "    year : int\n",
    "        Year to apply the function to.\n",
    "    column : str\n",
    "        Column of DataFrame to extract data from.\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # For data dealing with minimum values, obtain the lower of the two.\n",
    "        if column.startswith('min'):\n",
    "            df.loc[f'{year}-W52', column] = min([df.loc[f'{year}-W52', column], df.loc[f'{year}-W53', column]])\n",
    "            \n",
    "        # For data dealing with maximum values, obtain the higher of the two.\n",
    "        elif column.startswith('max'):\n",
    "            df.loc[f'{year}-W52', column] = max([df.loc[f'{year}-W52', column], df.loc[f'{year}-W53', column]])\n",
    "        \n",
    "        # For data dealing with totals, obtain the sum.\n",
    "        elif column.startswith('total'):\n",
    "            df.loc[f'{year}-W52', column] = df.loc[f'{year}-W52', column] + df.loc[f'{year}-W53', column]\n",
    "\n",
    "        # For data dealing with averages, obtain the mean.\n",
    "        elif column.startswith('mean'):\n",
    "            df.loc[f'{year}-W52', column] = (df.loc[f'{year}-W52', column] + df.loc[f'{year}-W53', column])/2\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    # Drop the now-irrelevant 53rd week.\n",
    "    # Note that, for all data not falling under any of the above if statements, the 53rd week is simply dropped.\n",
    "    df.drop(f'{year}-W53', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1044425-61df-4f4b-94bd-c9ef0ecf8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert a year column as well as week/month column, depending on the timeframe of the data.\n",
    "\n",
    "def insert_time_data(df=None, timeframe=None):\n",
    "    \n",
    "    '''   \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to be used in function.\n",
    "    timeframe : str\n",
    "        'Name' of resulting column.\n",
    "        Values should be 'week' or 'month'.\n",
    "    '''\n",
    "    \n",
    "    # Create a year column.\n",
    "    df['year'] = df.index.map(lambda x: int(x[:4]))\n",
    "    \n",
    "    # Create a column based on the timeframe specified.\n",
    "    df[timeframe] = df.index.map(lambda x: int(x[-2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82f18fd-0494-4c65-b330-2a7cc6af6e0a",
   "metadata": {},
   "source": [
    "---\n",
    "## Climate data\n",
    "---\n",
    "\n",
    "This daily climate datasets comprise of several different climate variables, of which we are only interested in the following: `total_rainfall`, `mean_temperature`, `minimum_temperature`, `maximum_temperature`, `mean_wind_speed` and `max_wind_speed`.\n",
    "\n",
    "There are many .csv files as each file represents a single month worth of daily data. Each file exists as a downloadable file on weather.gov.sg. However, as there is a different link for each file, instead of downloading the files manually, we will scrape the files instead.\n",
    "\n",
    "This is the only dataset that provides daily data, which must then further be compiled. The process of obtaining weekly and monthly data from it poses a few challenges, which we have decided to resolve to the best of our abilities.\n",
    "\n",
    "Firstly, to compile the data into weekly data, the `.isocalendar()` function will be used as it is the most accurate method. As `.isocalendar()` considers the first Monday of each year to be the first day of week 1, the days preceding that will be counted as part of the *previous year*. For example, the first Monday of 2022 is 3/1. After applying `.isocalendar()`, 1/1 and 2/1 will be classified under week 52, ie. the final week of 2021. \n",
    "\n",
    "Due to the structure of the dataset, this poses a problem - there is a `year` column originally in the dataset which we need for the sake of the index. To illustrate the problem using the example above:\n",
    "\n",
    "| Year | Date | Week (from `.isocalendar()` | Resulting year-week value for the index | Correct? |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 2022 | 3/1 | 1 | 2022-W01 | Yes |\n",
    "| 2022 | 2/1 | 52 | 2022-W52 | **No** |\n",
    "| 2022 | 1/1 | 52 | 2022-W52 | **No** |\n",
    "\n",
    "Therefore, after applying `.isocalendar()`, additional code will be written to ensure that these 'extra' days belong to the previous year, ie.:\n",
    "\n",
    "| Original Year | _Corrected_ Year | Date | Week (from `.isocalendar()` | Resulting year-week value for the index | Correct? |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 2022 | - | 3/1 | 1 | 2022-W01 | Yes |\n",
    "| 2022 | 2021 | 2/1 | 52 | 2022-W52 | **Yes** |\n",
    "| 2022 | 2021 | 1/1 | 52 | 2022-W52 | **Yes** |\n",
    "\n",
    "A manual adjustment will be made for the head and tail of the data:\n",
    "- 1/1/2012 will be categorised as week 52 of 2011, so it will be dropped;\n",
    "- 1/1/2023 will be categorised as week 52 of 2022, so it will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8469aaac-680f-4172-a477-e3328f1acb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create & clean weekly, monthly & yearly dataframes for temperature (mean, min, max) & rainfall data.\n",
    "\n",
    "def group_df_climate(df=None, groupby_col=None):\n",
    "    \n",
    "    '''   \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to be used in function.\n",
    "    groupby_col : str\n",
    "        Column in the DataFrame to group the data by.\n",
    "        Values should be 'year_week', 'year_month' or 'year'.\n",
    "    '''\n",
    "\n",
    "    # Create separate dataframes for each column.\n",
    "    # Rename columns whenever required.\n",
    "    df1 = df.groupby(groupby_col)[['daily_rainfall_total']].sum()\n",
    "    df1 = df1.rename(columns={'daily_rainfall_total': 'total_rainfall'})\n",
    "\n",
    "    df2 = df.groupby(groupby_col)['mean_temperature'].mean()\n",
    "    df3 = df.groupby(groupby_col)['minimum_temperature'].min()\n",
    "    df4 = df.groupby(groupby_col)[['minimum_temperature']].mean()\n",
    "    df4 = df4.rename(columns={'minimum_temperature': 'minimum_temperature_mean'})\n",
    "    \n",
    "    df5 = df.groupby(groupby_col)['maximum_temperature'].max()\n",
    "    df6 = df.groupby(groupby_col)[['maximum_temperature']].max()\n",
    "    df6 = df6.rename(columns={'maximum_temperature': 'maximum_temperature_mean'})\n",
    "    \n",
    "    df7 = df.groupby(groupby_col)['mean_wind_speed'].mean()\n",
    "    df8 = df.groupby(groupby_col)['max_wind_speed'].max()\n",
    "    df9 = df.groupby(groupby_col)[['max_wind_speed']].mean()\n",
    "    df9 = df9.rename(columns={'max_wind_speed': 'max_wind_speed_mean'})\n",
    "    \n",
    "    # Concatenate all the dataframes into one final dataframe.\n",
    "    df_new = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9], axis=1)\n",
    "    \n",
    "    # For weekly dataframes, apply the resolve_conflicts() function to...\n",
    "    # ...eliminate the issue of having a different number of weeks across years.\n",
    "    if groupby_col == 'year_week':\n",
    "        for year in range(2012,2023):\n",
    "            for column in df_new.columns:\n",
    "                resolve_conflicts(df=df_new, year=year, column=column)\n",
    "            \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62230ab7-2505-486c-8f7c-623e3bd3f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert a column reflecting cumulative rainfall by year.\n",
    "\n",
    "def get_cumulative_data(df=None, col=None, timeframe=None):\n",
    "    \n",
    "    '''   \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to be used in function.\n",
    "    col : str\n",
    "        Column of DataFrame to extract data from.\n",
    "    timeframe : str\n",
    "        'Name' of resulting column.\n",
    "        Values should be 'week' or 'month'.\n",
    "    '''\n",
    "\n",
    "    # Create a new column.\n",
    "    df[f'cumulative_{col}'] = 0\n",
    "\n",
    "    # Loop through each year in the dataset.\n",
    "    for year in df.year.unique():\n",
    "        \n",
    "        # Get the rows for the current year.\n",
    "        year_rows = df[df.year == year]\n",
    "\n",
    "        # Set the initial cumulative rainfall value to zero.\n",
    "        cumulative_total = 0\n",
    "\n",
    "        # Loop through each row in the year's data.\n",
    "        for index, row in year_rows.iterrows():\n",
    "            \n",
    "            # If the timeframe is 1, reset the cumulative total to zero.\n",
    "            if row[timeframe] == 1:\n",
    "                cumulative_total = 0\n",
    "\n",
    "            # Add the current week's rainfall to the cumulative total.\n",
    "            cumulative_total += row[col]\n",
    "\n",
    "            # Set the cumulative total rainfall value for the current row\n",
    "            df.at[index, f'cumulative_{col}'] = cumulative_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a743a3-9237-4180-baa3-26a7bfcd85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block to scrape and clean up the climate data.\n",
    "\n",
    "# Either read in climate.csv...\n",
    "try:\n",
    "    \n",
    "    df_climate = pd.read_csv('../data/climate.csv')\n",
    "    df_climate.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    \n",
    "# ...or create the file if it doesn't yet exist.\n",
    "except:\n",
    "    \n",
    "    # Create a list of years and months to match the filenames.\n",
    "    year_and_month = [''.join(i.split('-')[:2]) for i in pd.to_datetime([f'{year}' + f' 0{month}' \\\n",
    "                                                                         for year in range(1980,2023) \\\n",
    "                                                                         for month in range(1,13)]).astype(str)]\n",
    "    \n",
    "    # Include an additional month of data from 2023; rationale has been explained above.\n",
    "    year_and_month.append('202301')\n",
    "    \n",
    "    # Ensure that there are no repeated elements in the list.\n",
    "    year_and_month = list(set(year_and_month))\n",
    "    \n",
    "    # Using the list created above, create a list of URLs to scrape the .csv files from.\n",
    "    urls = [f'http://www.weather.gov.sg/files/dailydata/DAILYDATA_S24_{yyyymm}.csv' for yyyymm in year_and_month]\n",
    "    \n",
    "    # Partial filepath for the destination folder.\n",
    "    destination_folder = '../data/'\n",
    "    \n",
    "    # For each URL...\n",
    "    for url in urls:\n",
    "\n",
    "        # Extract just the variable components of the filename.\n",
    "        filename = url.split('/')[-1]\n",
    "\n",
    "        # Create a path based on the destination folder and the filename.\n",
    "        path = destination_folder + filename\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Using the path variable created above, attempt to read in the file if it already exists.\n",
    "            pd.read_csv(path, encoding='ANSI')\n",
    "\n",
    "        except:\n",
    "\n",
    "            # If the file does not exist, use BeautifulSoup to parse the website and download the .csv file.\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            content = response.content\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(content)\n",
    "                \n",
    "    # Create an empty dataframe to store the data in.\n",
    "    df_climate = pd.DataFrame()\n",
    "\n",
    "    # For each URL...\n",
    "    for url in urls:\n",
    "\n",
    "        # Extract just the variable components of the filename.\n",
    "        filename = url.split('/')[-1]\n",
    "\n",
    "        # Create a path based on the destination folder and the filename.\n",
    "        path = destination_folder + filename\n",
    "\n",
    "        # Read in the .csv files that were downloaded based on the code above.\n",
    "        df = pd.read_csv(path, encoding='ANSI')\n",
    "\n",
    "        # Create a new list containing the desired column names.\n",
    "        # The reason this is necessary is because the naming of certain columns in the .csv files are not consistent.\n",
    "        new_cols = [col.lower() for col in df.columns if len(col.split(' ')) < 2] + \\\n",
    "        ['_'.join(col.lower().split(' ')[:-1]) for col in df.columns if '(' in col]\n",
    "\n",
    "        # Rename the columns to ensure standardisation.\n",
    "        df.rename(columns={old_col: new_col for old_col, new_col in zip(df.columns, new_cols)}, \n",
    "                  inplace=True, \n",
    "                  errors='ignore')\n",
    "\n",
    "        # Concatenate the newly-created dataframe with df_climate.\n",
    "        df_climate = pd.concat([df_climate, df.loc[:, lambda x: x.columns.str.contains('station') == False]])\n",
    "\n",
    "        # Ensure that values are sorted by date and \n",
    "        df_climate = df_climate.sort_values(['year', 'month', 'day'], ascending=True).reset_index().drop('index', axis=1)\n",
    "\n",
    "        df_climate.to_csv('../data/climate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60b2d119-97a4-48aa-88be-f0de93bda69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the .isocalendar() function, create a 'week' column.\n",
    "\n",
    "df_climate['week'] = df_climate.apply(lambda x: date(x['year'], x['month'], x['day']).isocalendar()[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af44f296-beee-4fff-a809-e195980cd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As explained above, the code here ensures that the days preceding the 1st Monday of each year will be...\n",
    "# ...categorised under the preceding year.\n",
    "\n",
    "df_climate['year'] = df_climate.apply(lambda x: x.year - 1 if x.month == 1 and x.week > 50 else x.year, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53cd1d15-7c3b-45e2-9bfb-13543d0c1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all irrelevant values to 0 for the wind speed columns.\n",
    "\n",
    "for col in ['mean_wind_speed', 'max_wind_speed']:\n",
    "    df_climate[col] = pd.to_numeric(df_climate[col], errors='coerce')\n",
    "    df_climate[col] = df_climate[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a56fc7b7-6dfd-44ec-a8c7-5960c3414a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise values type to be float.\n",
    "\n",
    "df_climate = df_climate.loc[df_climate.year.between(1982,2023), :]\n",
    "\n",
    "for col in [\n",
    "    'mean_temperature', \n",
    "    'daily_rainfall_total', \n",
    "    'minimum_temperature', \n",
    "    'maximum_temperature', \n",
    "    'mean_wind_speed', \n",
    "    'max_wind_speed']:\n",
    "    \n",
    "    df_climate[col] = df_climate[col].map(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f83ad71-5bae-4cc1-a14a-4435ac8dff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate.reset_index().drop('index', axis=1).to_csv('../data/daily_weather_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9b7bde9-38eb-45ea-b58d-2f60c789910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As explained above, 1/1/2012 will be dropped & 1/1/2023 will be included in the data.\n",
    "\n",
    "df_climate = df_climate.loc[df_climate.year.between(2012,2022), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f210dd3-fbb5-40c0-ab4c-c223251f04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'year' & 'week' columns, create a 'year_week' column to be used as the index for the weekly dataframes.\n",
    "\n",
    "df_climate['year_week'] = df_climate.apply(lambda x: f'{x.year}-W0{x.week}' if x.week in range(1,10) \\\n",
    "                                           else f'{x.year}-W{x.week}', \n",
    "                                           axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55de4e84-d027-454f-8e45-7b279e55f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'year' & 'month' columns, create a 'year_month' column to be used as the index for the monthly dataframes.\n",
    "\n",
    "df_climate['year_month'] = df_climate.apply(lambda x: f'{x.year}-M0{x.month}' if x.month in range(1,10) \\\n",
    "                                            else f'{x.year}-M{x.month}',\n",
    "                                            axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44a45949-0625-4c0f-9fa1-6f18c5e771ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the group_df_climate function, create weekly dataframe for temperature & rainfall data.\n",
    "\n",
    "df_climate_weekly = group_df_climate(df=df_climate, groupby_col='year_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70acc4a7-63df-471b-8cd3-add89e19ee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the group_df_climate function, create monthly dataframe for temperature & rainfall data.\n",
    "\n",
    "df_climate_monthly = group_df_climate(df=df_climate, groupby_col='year_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc891ea0-feb6-4d3c-962b-c007533a9366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the group_df_climate function, create yearly dataframe for temperature & rainfall data.\n",
    "\n",
    "df_climate_yearly = group_df_climate(df=df_climate, groupby_col='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59740d-e78e-4ca1-85cc-920332ce6372",
   "metadata": {},
   "source": [
    "---\n",
    "## Dengue data\n",
    "---\n",
    "\n",
    "Dengue data is extracted from a dataset comprising weekly statistics for infectious diseases, including Dengue Fever & Dengue Haemorrhagic Fever. Both of these data will be merged and used for the purposes of this project.\n",
    "\n",
    "Unlike the (daily) climate data, this dataset does not include 'year', 'month' and 'week' columns. The year and week can be obtained from the index, though it is unknown how the week numbering is derived, but the month will have to be inferred using the `datetime.strptime()` function. Specifically, the function takes in the following 'date' format: `%Y-%W-%w`, where `%Y` represents the year, `%W` represents the week number, and `%w` represents the day of week. In our case, we will set `%w` to `1`, which stands for Monday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f25b9d2-69fe-4b71-a6c9-cbbe4e103ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the month given a year and week.\n",
    "\n",
    "def get_month_from_week(year, week):\n",
    "    \n",
    "    '''   \n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Year to be used in function.\n",
    "    week : int\n",
    "        Week to be used in function.\n",
    "    '''\n",
    "    \n",
    "    date = datetime.strptime(f'{year}-{week}-1', \"%Y-%W-%w\")\n",
    "    \n",
    "    return date.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64867eab-4f7c-44f8-ae40-039795b91c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in weekly infectious diseases data.\n",
    "\n",
    "df_dengue = pd.read_csv('../data/weekly-infectious-disease-bulletin-cases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f21c2f5-cfa7-4d46-9bdf-f368069fbcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant information & merge relevant information.\n",
    "\n",
    "df_dengue = df_dengue.loc[df_dengue.disease.map(lambda x: (x=='Dengue Fever') | (x=='Dengue Haemorrhagic Fever'))]\\\n",
    ".reset_index()\\\n",
    ".drop('index', axis=1)\\\n",
    ".groupby('epi_week')[['epi_week', 'no._of_cases']]\\\n",
    ".sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c94ff58-5f82-4851-8448-9a79527a5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column to be more informative.\n",
    "\n",
    "df_dengue.rename(columns={'no._of_cases': 'dengue_cases'}, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22fc1c60-8eb8-46eb-843c-bd79786ef231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the index, create a 'year' column to be used as the index for the yearly dataframe.\n",
    "\n",
    "df_dengue['year'] = df_dengue.index.map(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6898b1e-922b-48b5-bbde-c378612a146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the index, create a 'week' column.\n",
    "\n",
    "df_dengue['week'] = df_dengue.index.map(lambda x: int(x[-2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eba1ec05-2424-471e-887a-6045ab7cc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the resolve_conflicts() function, standardise number of weeks to be 52 per year.\n",
    "\n",
    "for year in range(2012,2023):\n",
    "    resolve_conflicts(df=df_dengue, year=year, column='dengue_cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e409de5-ce3d-4a79-9267-ecf0fe18e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the get_month_from_week(), create a 'month' column.\n",
    "\n",
    "df_dengue['month'] = df_dengue.apply(lambda x: get_month_from_week(x.year, x.week), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d260ede-3ad5-4116-ae37-db7b4e7d4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'year' & 'month' columns, create a 'year_month' column to be used as the index for the monthly dataframe.\n",
    "\n",
    "df_dengue['year_month'] = df_dengue.apply(lambda x: f'{x.year}-M0{x.month}' if x.month in range(1,10) \\\n",
    "                                          else f'{x.year}-M{x.month}',\n",
    "                                          axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0efd7043-a16d-44e5-83f7-5c3695133e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weekly dataframe.\n",
    "\n",
    "df_dengue_weekly = df_dengue[['dengue_cases']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f48de28-9d55-4344-bdcb-4fa143b9c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create monthly dataframe.\n",
    "\n",
    "df_dengue_monthly = df_dengue.groupby('year_month')[['dengue_cases']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b265dd2-96f6-4fe0-bd9a-568adc61c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create yearly dataframe.\n",
    "\n",
    "df_dengue_yearly = df_dengue.groupby('year')[['dengue_cases']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebbbe61-e6f4-45b2-b5b3-f89f5346475d",
   "metadata": {},
   "source": [
    "---\n",
    "## Google search trends data\n",
    "---\n",
    "\n",
    "As mentioned above, Google search trends data does not make use of absolute numbers that directly show the number of searches, but rather an 'index' of sorts to represent the ***popularity*** of the term relative to highest point on the chart for the given region (Singapore) and time period (2012 to 2022). To avoid inaccuracy of representations, we will only make use of available (weekly & monthly) data, and avoid using them to 'create' other (yearly) data.\n",
    "\n",
    "As with all the other datasets, the key challenge here is to extract a week number given the available data. In the case of the trends dataset, the source (Google) considers 1/1/2012 to be the 1st day of the year 2012 and 31/12/2012 to be the last day of 2022. (*Note: while this seems obvious, it is not always the case, since there are 365 days in a year, which is a number that is not perfectly divisible by 7; ie. the selected time period happens to coincide nicely with conventional ways of thinking of days in a year.*)\n",
    "\n",
    "In this case, our job here is to assign a week number to each row of the DataFrame conditional on its year. Then, as mentioned above, the 53rd week of years with 53 weeks will be dropped due to the nature of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2ec146d-1df1-424c-854d-0527efa3364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read in and clean each trends dataset.\n",
    "\n",
    "def create_df_trends(path='../data/interest_trend_2012_to_2016.csv'):\n",
    "    \n",
    "    '''   \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Directory path to read in the .csv file from.\n",
    "    '''\n",
    "    \n",
    "    # Read in weekly google search trends.\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Drop irrelevant first row.\n",
    "    df = df.iloc[1:, :]\n",
    "    \n",
    "    # Rename column to be more informative.\n",
    "    df.rename(columns={'Category: All categories': 'interest'}, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Convert values to int.\n",
    "    df['interest'] = df.interest.map(lambda x: int(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d4b9727-1d2c-4b86-94d0-7e60867653a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the create_df_trends() function, create weekly dataset from 3 separate .csv files. \n",
    "\n",
    "df_trends_weekly = pd.concat([create_df_trends(path=f'../data/interest_trend_{filename}.csv') \\\n",
    "                              for filename in ['2012_to_2016', '2017_to_2021', '2022']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b66efa9-65de-4e96-9365-b5cd63ec66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the index, create a 'year' column.\n",
    "\n",
    "df_trends_weekly['year'] = df_trends_weekly.index.map(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea955ea4-7e25-46e7-a5f9-3888f0764830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the insert_week_col() function, create a 'week' column.\n",
    "\n",
    "insert_week_col(df=df_trends_weekly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d3a6bb4-3dee-469f-8289-07414a524161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'year' & 'week' columns, overwrite the index for the dataframe to follow the standardised format.\n",
    "\n",
    "df_trends_weekly.index = df_trends_weekly.apply(lambda x: f'{x.year}-W0{x.week}' if x.week in range(1,10) \\\n",
    "                                                       else f'{x.year}-W{x.week}', \n",
    "                                                       axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffec7021-0528-40bc-acb2-e95475c96f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the resolve_conflicts() function, standardise number of weeks to be 52 per year.\n",
    "\n",
    "for year in range(2012,2023):\n",
    "    resolve_conflicts(df=df_trends_weekly, year=year, column='interest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80296f2e-2b9c-4fee-9d4d-949849fbaa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the create_df_trends() function, create monthly dataset. \n",
    "\n",
    "df_trends_monthly = create_df_trends(path='../data/interest_trend_2012_to_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db7d9992-c04b-42dc-b066-e503847dcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the index, create a 'year' column.\n",
    "\n",
    "df_trends_monthly['year'] = df_trends_monthly.index.map(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8e80bb9-dcf7-4bbc-8b8b-6a77763475f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the index, create a 'month' column.\n",
    "\n",
    "df_trends_monthly['month'] = df_trends_monthly.index.map(lambda x: int(x[-2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb484c2d-0aff-480b-a0db-2ed9ecc6242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'year' & 'month' columns, overwrite the index for the dataframe to follow the standardised format.\n",
    "\n",
    "df_trends_monthly.index = df_trends_monthly.apply(lambda x: f'{x.year}-M0{x.month}' if x.month in range(1,10) \\\n",
    "                                                  else f'{x.year}-M{x.month}',\n",
    "                                                  axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee733c3-2087-4edd-a712-d9a8afd39a2f",
   "metadata": {},
   "source": [
    "---\n",
    "## Travel data\n",
    "---\n",
    "\n",
    "Comprising both inbound & outbound, travel data exists at the monthly level. Since we will be dealing mostly with weekly data, some liberties will be taken here to 'convert' monthly travel data into weekly travel data by dividing each data point into the relevant number of weeks. As noted above, this will compromise on accuracy, but not to the extent where the data will be rendered unuseable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "754cb58d-87e3-48f8-849f-e86c7f222b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weeks_in_month(year, month):\n",
    "    \n",
    "    '''   \n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Year to be used in function.\n",
    "    week : int\n",
    "        Week to be used in function.\n",
    "    '''\n",
    "    \n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    \n",
    "    last_week = datetime(year, month, last_day).strftime(\"%W\")\n",
    "    \n",
    "    return int(last_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dac38cb2-ab65-4868-9b55-3fdf855b5968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of months where the keys are the months as shown in the travel datasets, and...\n",
    "# ...the values are their numerical representations which we need for standardisation purposes.\n",
    "\n",
    "months = {word: num for word, num in zip([mth[:3] for mth in list(calendar.month_name)[1:]], range(1,13))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92932f7d-f767-4ec1-8fd7-587796cdba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_travel_data(path='../data/monthly_arrivals.csv'):\n",
    "    \n",
    "    '''   \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Directory path to read in the .csv file from.\n",
    "    '''\n",
    "\n",
    "    if 'arrivals' in path:\n",
    "        main_col = 'inbound'\n",
    "        \n",
    "    else:\n",
    "        main_col = 'outbound'\n",
    "    \n",
    "    # Read in data.\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Drop unnecessary first online & rename column to be more accurate.\n",
    "    df = pd.DataFrame(df.loc[0, :]).drop('Data Series').rename(columns={0: main_col})\n",
    "    \n",
    "    # Using the index, create a 'year' column.\n",
    "    df['year'] = df.index.map(lambda x: int(x[:4]))\n",
    "    \n",
    "    # Using the index & months dictionary, create a 'month' column.\n",
    "    df['month'] = df.index.map(lambda x: months[x[5:8]])\n",
    "    \n",
    "    # Sort the data, first by year, then month, in ascending order.\n",
    "    df.sort_values(['year', 'month'], inplace=True)\n",
    "    \n",
    "    # Using the 'year' & 'month' columns, create a 'year_month' column to be used as the index for the monthly dataframe.\n",
    "    df['year_month'] = df.apply(lambda x: f'{x.year}-M0{x.month}' if x.month in range(1,10) \\\n",
    "                                else f'{x.year}-M{x.month}',\n",
    "                                axis=1)\n",
    "    \n",
    "    # Using the get_weeks_in_month() function, create an 'n_weeks_consec' column that reflects...\n",
    "    # ...the cumulative number of weeks in each month of a given year.\n",
    "    df['n_weeks_consec'] = df.apply(lambda x: get_weeks_in_month(x.year, x.month), axis=1)\n",
    "    \n",
    "    # Set the 53rd week of any given year to be the 52nd week for standardisation purposes.\n",
    "    df['n_weeks_consec'] = df.n_weeks_consec.map(lambda x: x if x <= 52 else 52)\n",
    "    \n",
    "    # Set the new column, 'n_weeks', to have the same values as the column 'n_weeks_consec'.\n",
    "    # This is done to ensure that the first month of each year gets the correct number of weeks.\n",
    "    df['n_weeks'] = df['n_weeks_consec']\n",
    "\n",
    "    # These variables will be used in the for loop below.\n",
    "    prev_year = None\n",
    "    prev_week = None\n",
    "\n",
    "    # Iterate over each row of the dataframe.\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        # If the 'year' value for the current row is different from the 'year' value for the previous row...\n",
    "        if prev_year is not None and row['year'] != prev_year:\n",
    "\n",
    "            # ...restart the numbering for the weeks.\n",
    "            prev_week = None\n",
    "\n",
    "        # If it is the start of a new year, retain the current 'n_weeks' value.\n",
    "        # If not, set the current 'n_weeks' value to be the difference between the current & previous value.\n",
    "        if prev_week is not None:\n",
    "\n",
    "            df.at[idx, 'n_weeks'] = row['n_weeks'] - prev_week\n",
    "\n",
    "        # Update the previous year and previous week values to be the current values.\n",
    "        prev_year = df.at[idx, 'year']\n",
    "        prev_week = df.at[idx, 'n_weeks_consec']\n",
    "    \n",
    "    # Create an empty dataframe that has the same columns as the original df.\n",
    "    df_weekly = pd.DataFrame(np.arange(6).reshape(1, 6), columns=df.columns).drop(0)\n",
    "    \n",
    "    # For each row in the original df, create X number of duplicate rows, where X = the value in the 'n_weeks' column.\n",
    "    # Append these rows to df_weekly to obtain a 'weekly' dataframe.\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        df_weekly = pd.concat([df_weekly, pd.concat([pd.DataFrame(df.loc[idx, :]).transpose() \\\n",
    "                                                     for i in range(df.loc[idx, 'n_weeks'])])])\n",
    "    \n",
    "    # Obtain 'weekly' data by dividing the actual number ('inbound' column) by the number of weeks ('n_weeks' column).\n",
    "    df_weekly[main_col] = df_weekly.apply(lambda x: round(x[main_col]/x.n_weeks), axis=1)\n",
    "    \n",
    "    # Reset df_weekly's index to avoid having duplicated values in the index.\n",
    "    df_weekly = df_weekly.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    # Ensure that all columns, except for the 'year_month' column, are in numerical format.\n",
    "    # This must be done to ensure that the next portion of the code can be successfully carried out.\n",
    "    for col in df_weekly.columns:\n",
    "        \n",
    "        if col != 'year_month':\n",
    "            \n",
    "            df_weekly[col] = df_weekly[col].map(lambda x: int(x))\n",
    "    \n",
    "    # Using the insert_week_col() function, create a 'week' column.\n",
    "    insert_week_col(df=df_weekly)\n",
    "    \n",
    "    # Using the 'year' & 'week' columns, overwrite the index for the dataframe to follow the standardised format.\n",
    "    df_weekly.index = df_weekly.apply(lambda x: f'{x.year}-W0{x.week}' if x.week in range(1,10) \\\n",
    "                                      else f'{x.year}-W{x.week}',\n",
    "                                      axis=1)\n",
    "    \n",
    "    # Create the finalised weekly version of the travel data.\n",
    "    df_weekly = df_weekly[[main_col]]\n",
    "    \n",
    "    # Create the finalised monthly version of the travel data.\n",
    "    df_monthly = df.groupby('year_month')[[main_col]].sum()\n",
    "    \n",
    "    # Create the finalised yearly version of the travel data.\n",
    "    df_yearly = df.groupby('year')[[main_col]].sum()\n",
    "    \n",
    "    return [df_weekly, df_monthly, df_yearly]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a84c6b4-e043-48ac-8777-538ca124eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the get_travel_data() function, create dataframes for travel data.\n",
    "\n",
    "df_travel_all = get_travel_data(path='../data/monthly_arrivals.csv') + \\\n",
    "get_travel_data(path='../data/monthly_departures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7a2af9b-edc9-44fc-938c-4faab79c11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weekly, monthly & yearly travel travel dataframes from df_travel_all.\n",
    "\n",
    "df_travel_weekly = pd.concat([df_travel_all[i] for i in [0, 3]], axis=1)\n",
    "df_travel_monthly = pd.concat([df_travel_all[i] for i in [1, 4]], axis=1)\n",
    "df_travel_yearly = pd.concat([df_travel_all[i] for i in [2, 5]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4903b1e-641b-4ca7-9688-db7f4422ef03",
   "metadata": {},
   "source": [
    "---\n",
    "## Combined weekly, monthly & yearly data\n",
    "---\n",
    "\n",
    "**Notes**\n",
    "- Yearly data is not available for Google search trends\n",
    "- Weekly data for outbound & inbound travel is simply monthly data divided by the appropriate number of weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c182475b-636e-4dc1-b572-ab3cc12de063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined weekly dataframe with additional columns - year, week, cumulative total rainfall.\n",
    "\n",
    "df_weekly = pd.concat([\n",
    "    df_climate_weekly,\n",
    "    df_dengue_weekly,\n",
    "    df_trends_weekly[['interest']],\n",
    "    df_travel_weekly],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "insert_time_data(df=df_weekly, timeframe='week')\n",
    "\n",
    "for col in ['total_rainfall', 'dengue_cases']:\n",
    "    get_cumulative_data(df=df_weekly, col=col, timeframe='week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7be1611-d238-44dd-b135-2c08db49aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined monthly dataframe with additional columns - year, month, cumulative total rainfall.\n",
    "\n",
    "df_monthly = pd.concat([\n",
    "    df_climate_monthly,\n",
    "    df_dengue_monthly,\n",
    "    df_trends_monthly[['interest']],\n",
    "    df_travel_monthly],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "insert_time_data(df=df_monthly, timeframe='month')\n",
    "\n",
    "for col in ['total_rainfall', 'dengue_cases']:\n",
    "    get_cumulative_data(df=df_monthly, col=col, timeframe='month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de467dd4-c046-483a-9653-1dbe6a30fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined yearly dataframe.\n",
    "\n",
    "df_yearly = pd.concat([\n",
    "    df_climate_yearly,\n",
    "    df_dengue_yearly,\n",
    "    df_travel_yearly],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109bde5-d365-48e8-91b1-03b177beb1b3",
   "metadata": {},
   "source": [
    "---\n",
    "## Cumulative weekly & monthly data\n",
    "---\n",
    "\n",
    "Cumulative weekly data refers to combined weekly data spanning 2012 to 2022. For example, week 1 refers to the combined week 1 data from all 11 years.\n",
    "\n",
    "Cumulative monthly data refers to combined monthly data spanning 2012 to 2022. For example, Jan refers to the combined Jan data from all 11 years.\n",
    "\n",
    "As always, Google search trends will be left out from cumulative data for reasons expounded upon above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "530325d4-4ff0-4730-bf5c-0cbf6224150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create an additional Dataframe that reflects cumulative weekly/monthly data from 2012 to 2022.\n",
    "\n",
    "def create_cumulative_df(df=None, timeframe=None):\n",
    "    \n",
    "    '''   \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to be used in function.\n",
    "    timeframe : str\n",
    "        Values should be 'week' or 'month'.\n",
    "    '''\n",
    "    \n",
    "    df_cumulative = pd.DataFrame()\n",
    "    \n",
    "    df_cumulative['mean_temperature'] = df.groupby(df[timeframe]).mean_temperature.mean()\n",
    "    df_cumulative['minimum_temperature'] = df.groupby(df[timeframe]).minimum_temperature.mean()\n",
    "    df_cumulative['minimum_temperature_mean'] = df.groupby(df[timeframe]).minimum_temperature_mean.mean()\n",
    "    df_cumulative['maximum_temperature'] = df.groupby(df[timeframe]).maximum_temperature.mean()\n",
    "    df_cumulative['maximum_temperature_mean'] = df.groupby(df[timeframe]).maximum_temperature_mean.mean()\n",
    "    df_cumulative['total_rainfall'] = df.groupby(df[timeframe]).total_rainfall.sum()\n",
    "    df_cumulative['cumulative_total_rainfall'] = df.groupby(df[timeframe]).cumulative_total_rainfall.sum()\n",
    "    df_cumulative['mean_wind_speed'] = df.groupby(df[timeframe]).mean_wind_speed.mean()\n",
    "    df_cumulative['max_wind_speed'] = df.groupby(df[timeframe]).max_wind_speed.max()\n",
    "    df_cumulative['max_wind_speed_mean'] = df.groupby(df[timeframe]).max_wind_speed.mean()\n",
    "    df_cumulative['dengue_cases'] = df.groupby(df[timeframe]).dengue_cases.sum()\n",
    "    df_cumulative['interest'] = df.groupby(df[timeframe]).interest.mean()\n",
    "    df_cumulative['inbound'] = df.groupby(df[timeframe]).inbound.sum()\n",
    "    df_cumulative['outbound'] = df.groupby(df[timeframe]).outbound.sum()\n",
    "    \n",
    "    return df_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fcf8432-e03c-4104-aae0-e784ff63ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the create_cumulative_df function, create a Dataframe that reflects cumulative weekly data.\n",
    "\n",
    "df_weekly_cumulative = create_cumulative_df(df=df_weekly, timeframe='week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bb0c44f-a627-424d-8e7e-3af590bbcf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the create_cumulative_df function, create a Dataframe that reflects cumulative monthly data.\n",
    "\n",
    "df_monthly_cumulative = create_cumulative_df(df=df_monthly, timeframe='month')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3662519b-53db-40fa-9b1b-9ce486c8e187",
   "metadata": {},
   "source": [
    "---\n",
    "## Export data\n",
    "---\n",
    "\n",
    "A total of 5 dataframes will be exported as .csv files for use in the other notebooks:\n",
    "- `df_weekly`\n",
    "- `df_monthly`\n",
    "- `df_yearly`\n",
    "- `df_weekly_cumulative`\n",
    "- `df_monthly_cumulative`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "081ed85f-7414-474b-ac8a-1794073402a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframes as .csv files.\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "df_weekly.to_csv(path+'weekly_data.csv')\n",
    "df_monthly.to_csv(path+'monthly_data.csv')\n",
    "df_yearly.to_csv(path+'yearly_data.csv')\n",
    "df_weekly_cumulative.to_csv(path+'weekly_data_cumulative.csv')\n",
    "df_monthly_cumulative.to_csv(path+'monthly_data_cumulative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5f42a-9b0c-48d7-af29-23faad5680ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi-sg]",
   "language": "python",
   "name": "conda-env-dsi-sg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
